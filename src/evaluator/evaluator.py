#!/usr/bin/env python3
"""
í‰ê°€ ì—”ì§„

YAML ì‹œí—˜ íŒŒì¼ì„ ë¡œë“œí•˜ê³  AI ëª¨ë¸ë¡œ ë¬¸ì œë¥¼ í’€ì–´ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
"""

import yaml
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

from .base_model import BaseModel, ModelResponse
from .models import (
    OpenAIModel,
    AnthropicModel,
    GoogleModel,
    UpstageModel,
    PerplexityModel
)


class Evaluator:
    """í‰ê°€ ì—”ì§„"""
    
    def __init__(self, models_config_path: Optional[str] = None):
        """
        Args:
            models_config_path: models.json íŒŒì¼ ê²½ë¡œ
        """
        self.models_config_path = models_config_path or "models/models.json"
        self.models: Dict[str, BaseModel] = {}
        self._load_models_config()
    
    def _load_models_config(self):
        """models.jsonì—ì„œ ëª¨ë¸ ì„¤ì • ë¡œë“œ"""
        import json
        
        if not Path(self.models_config_path).exists():
            print(f"âš ï¸  ëª¨ë¸ ì„¤ì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.models_config_path}")
            return
        
        with open(self.models_config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        print(f"ğŸ“‹ ëª¨ë¸ ì„¤ì • ë¡œë“œ: {len(config.get('models', []))}ê°œ ëª¨ë¸")
    
    def create_model(self, provider: str, model_name: str, api_key: str, **kwargs) -> BaseModel:
        """ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        
        Args:
            provider: ì œê³µì (openai, anthropic, google, upstage, perplexity)
            model_name: ëª¨ë¸ ì´ë¦„
            api_key: API í‚¤
            **kwargs: ì¶”ê°€ ì„¤ì •
        
        Returns:
            BaseModel ì¸ìŠ¤í„´ìŠ¤
        """
        provider_map = {
            'openai': OpenAIModel,
            'anthropic': AnthropicModel,
            'google': GoogleModel,
            'upstage': UpstageModel,
            'perplexity': PerplexityModel
        }
        
        model_class = provider_map.get(provider.lower())
        if not model_class:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì œê³µì: {provider}")
        
        return model_class(api_key=api_key, model_name=model_name, **kwargs)
    
    def load_exam(self, exam_path: str) -> Dict[str, Any]:
        """ì‹œí—˜ YAML íŒŒì¼ ë¡œë“œ
        
        Args:
            exam_path: YAML íŒŒì¼ ê²½ë¡œ
        
        Returns:
            ì‹œí—˜ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        with open(exam_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def _solve_single_question(self, model: BaseModel, question: Dict[str, Any], index: int) -> Dict[str, Any]:
        """ë‹¨ì¼ ë¬¸ì œ í’€ì´ (ë³‘ë ¬ ì²˜ë¦¬ìš©)

        Args:
            model: AI ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
            question: ë¬¸ì œ ë°ì´í„°
            index: ë¬¸ì œ ì¸ë±ìŠ¤ (1ë¶€í„° ì‹œì‘)

        Returns:
            ë¬¸ì œ í’€ì´ ê²°ê³¼
        """
        q_id = question.get('question_id', f'q{index}')
        q_num = question.get('question_number', index)
        q_text = question.get('question_text', '')
        choices = question.get('choices', [])
        passage = question.get('passage')
        correct_answer = question.get('correct_answer')
        points = question.get('points', 2)

        # ë¬¸ì œ í’€ì´
        response = model.solve_question(
            question_text=q_text,
            choices=choices,
            passage=passage
        )

        # ì •ë‹µ í™•ì¸
        is_correct = (response.answer == correct_answer) if correct_answer else None

        # ê²°ê³¼ ì €ì¥
        result = {
            'question_id': q_id,
            'question_number': q_num,
            'answer': response.answer,
            'correct_answer': correct_answer,
            'is_correct': is_correct,
            'reasoning': response.reasoning,
            'time_taken': round(response.time_taken, 2),
            'points': points,
            'earned_points': points if is_correct else 0,
            'success': response.success,
            'error': response.error
        }

        return result

    def evaluate_exam(
        self,
        exam_path: str,
        model: BaseModel,
        output_path: Optional[str] = None,
        parallel: bool = False,
        max_workers: int = 10
    ) -> Dict[str, Any]:
        """ì‹œí—˜ í‰ê°€ ì‹¤í–‰

        Args:
            exam_path: ì‹œí—˜ YAML íŒŒì¼ ê²½ë¡œ
            model: AI ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
            output_path: ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
            parallel: ë³‘ë ¬ ì²˜ë¦¬ ì—¬ë¶€ (ê¸°ë³¸: False)
            max_workers: ìµœëŒ€ ë™ì‹œ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìˆ˜ (ê¸°ë³¸: 10)

        Returns:
            í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print(f"\n{'='*100}")
        print(f"ğŸ¯ í‰ê°€ ì‹œì‘")
        print(f"{'='*100}")
        print(f"ğŸ“„ ì‹œí—˜: {exam_path}")
        print(f"ğŸ¤– ëª¨ë¸: {model.model_name}")
        if parallel:
            print(f"âš¡ ë³‘ë ¬ ì²˜ë¦¬: {max_workers}ê°œ ìŠ¤ë ˆë“œ")
        print(f"{'='*100}\n")

        # ì‹œí—˜ ë¡œë“œ
        exam_data = self.load_exam(exam_path)
        exam_id = exam_data.get('exam_id', 'unknown')
        questions = exam_data.get('questions', [])

        print(f"ğŸ“Š ë¬¸ì œ ìˆ˜: {len(questions)}ê°œ\n")

        start_time = time.time()

        if parallel:
            # ë³‘ë ¬ ì²˜ë¦¬
            results = []
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # ëª¨ë“  ë¬¸ì œë¥¼ ë™ì‹œì— ì œì¶œ
                future_to_question = {
                    executor.submit(self._solve_single_question, model, q, i): (i, q)
                    for i, q in enumerate(questions, 1)
                }

                # ì™„ë£Œë˜ëŠ” ëŒ€ë¡œ ê²°ê³¼ ìˆ˜ì§‘
                for future in as_completed(future_to_question):
                    i, question = future_to_question[future]
                    try:
                        result = future.result()
                        results.append(result)

                        # ì§„í–‰ ìƒí™© ì¶œë ¥
                        q_num = result['question_number']
                        points = result['points']
                        is_correct = result['is_correct']
                        answer = result['answer']
                        correct_answer = result['correct_answer']
                        time_taken = result['time_taken']
                        success = result['success']

                        status = "âœ…" if is_correct else "âŒ"
                        if not success:
                            status = "âš ï¸"

                        print(f"ğŸ“ ë¬¸ì œ {q_num}ë²ˆ ({points}ì ): {status} ë‹µ: {answer}ë²ˆ (ì •ë‹µ: {correct_answer}ë²ˆ) - {time_taken:.2f}ì´ˆ")

                        if not success:
                            print(f"   âš ï¸  ì—ëŸ¬: {result['error']}")

                    except Exception as e:
                        print(f"âŒ ë¬¸ì œ {i}ë²ˆ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

            # ë¬¸ì œ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬
            results.sort(key=lambda x: x['question_number'])

        else:
            # ìˆœì°¨ ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)
            results = []
            for i, question in enumerate(questions, 1):
                q_num = question.get('question_number', i)
                points = question.get('points', 2)

                print(f"ğŸ“ ë¬¸ì œ {q_num}ë²ˆ ({points}ì )...", end=' ')

                result = self._solve_single_question(model, question, i)
                results.append(result)

                # ì§„í–‰ ìƒí™© ì¶œë ¥
                status = "âœ…" if result['is_correct'] else "âŒ"
                if not result['success']:
                    status = "âš ï¸"
                print(f"{status} ë‹µ: {result['answer']}ë²ˆ (ì •ë‹µ: {result['correct_answer']}ë²ˆ) - {result['time_taken']:.2f}ì´ˆ")

                if not result['success']:
                    print(f"   âš ï¸  ì—ëŸ¬: {result['error']}")

        total_time = time.time() - start_time

        # í†µê³„ ê³„ì‚°
        max_score = sum(r['points'] for r in results)
        correct_count = sum(1 for r in results if r['is_correct'])
        total_score = sum(r['earned_points'] for r in results)
        
        # ìµœì¢… ê²°ê³¼
        accuracy = (correct_count / len(questions) * 100) if questions else 0
        score_rate = (total_score / max_score * 100) if max_score > 0 else 0
        
        print(f"\n{'='*100}")
        print(f"ğŸ“Š í‰ê°€ ì™„ë£Œ")
        print(f"{'='*100}")
        print(f"ì •ë‹µë¥ : {correct_count}/{len(questions)} ({accuracy:.1f}%)")
        print(f"ì ìˆ˜: {total_score}/{max_score}ì  ({score_rate:.1f}%)")
        print(f"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {total_time:.1f}ì´ˆ")
        if parallel:
            avg_time_per_question = sum(r['time_taken'] for r in results) / len(results) if results else 0
            speedup = (avg_time_per_question * len(results)) / total_time if total_time > 0 else 1
            print(f"âš¡ ì†ë„ í–¥ìƒ: {speedup:.1f}ë°° (ë³‘ë ¬ ì²˜ë¦¬)")
        print(f"{'='*100}\n")
        
        # ê²°ê³¼ ë°ì´í„° êµ¬ì„±
        result_data = {
            'exam_id': exam_id,
            'exam_title': exam_data.get('title', ''),
            'subject': exam_data.get('subject', ''),
            'model_name': model.model_name,
            'evaluated_at': datetime.now().isoformat(),
            'summary': {
                'total_questions': len(questions),
                'correct_answers': correct_count,
                'accuracy': round(accuracy, 2),
                'total_score': total_score,
                'max_score': max_score,
                'score_rate': round(score_rate, 2)
            },
            'results': results
        }
        
        # ê²°ê³¼ ì €ì¥
        if output_path is None:
            # ìë™ ê²½ë¡œ ìƒì„±: results/{exam_id}/{model_name}.yaml
            results_dir = Path("results") / exam_id
            results_dir.mkdir(parents=True, exist_ok=True)
            output_path = results_dir / f"{model.model_name.replace('/', '-')}.yaml"
        else:
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            yaml.dump(result_data, f, allow_unicode=True, default_flow_style=False, sort_keys=False)
        
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}\n")
        
        return result_data
    
    def evaluate_with_all_models(
        self,
        exam_path: str,
        models_to_use: Optional[List[str]] = None
    ) -> Dict[str, Dict[str, Any]]:
        """ëª¨ë“  ëª¨ë¸ë¡œ ì‹œí—˜ í‰ê°€
        
        Args:
            exam_path: ì‹œí—˜ YAML íŒŒì¼ ê²½ë¡œ
            models_to_use: ì‚¬ìš©í•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ì „ì²´)
        
        Returns:
            {model_name: result_data} ë”•ì…”ë„ˆë¦¬
        """
        import json
        
        # ëª¨ë¸ ì„¤ì • ë¡œë“œ
        with open(self.models_config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        models = config.get('models', [])
        if models_to_use:
            models = [m for m in models if m['name'] in models_to_use]
        
        print(f"\nğŸš€ ì „ì²´ ëª¨ë¸ í‰ê°€ ì‹œì‘ ({len(models)}ê°œ ëª¨ë¸)")
        print(f"ğŸ“„ ì‹œí—˜: {exam_path}\n")
        
        results = {}
        
        for model_config in models:
            provider = model_config['provider']
            model_name = model_config['name']
            model_id = model_config.get('model_id', model_name)  # model_idê°€ ì—†ìœ¼ë©´ name ì‚¬ìš©
            
            # API í‚¤ ê°€ì ¸ì˜¤ê¸°
            api_key_var = f"{provider.upper()}_API_KEY"
            api_key = os.getenv(api_key_var)
            
            if not api_key:
                print(f"âš ï¸  {model_name} ìŠ¤í‚µ: {api_key_var} í™˜ê²½ë³€ìˆ˜ ì—†ìŒ")
                continue
            
            try:
                # ëª¨ë¸ ìƒì„± (model_idë¥¼ ì „ë‹¬)
                model = self.create_model(
                    provider=provider,
                    model_name=model_id,  # APIìš© ì‹¤ì œ model_id ì „ë‹¬
                    api_key=api_key
                )
                # í‘œì‹œìš© ì´ë¦„ì€ model_name ì‚¬ìš©
                model.display_name = model_name
                
                # í‰ê°€ ì‹¤í–‰
                result = self.evaluate_exam(exam_path, model)
                results[model_name] = result
                
            except Exception as e:
                print(f"âŒ {model_name} í‰ê°€ ì‹¤íŒ¨: {e}")
                continue
        
        print(f"\nğŸ‰ ì „ì²´ í‰ê°€ ì™„ë£Œ! ({len(results)}/{len(models)}ê°œ ì„±ê³µ)\n")
        
        return results

