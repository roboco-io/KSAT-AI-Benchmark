#!/usr/bin/env python3
"""
í‰ê°€ ì—”ì§„

YAML ì‹œí—˜ íŒŒì¼ì„ ë¡œë“œí•˜ê³  AI ëª¨ë¸ë¡œ ë¬¸ì œë¥¼ í’€ì–´ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
"""

import yaml
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import logging
import json

from .base_model import BaseModel, ModelResponse
from .models import (
    OpenAIModel,
    AnthropicModel,
    GoogleModel,
    UpstageModel,
    PerplexityModel
)


class Evaluator:
    """í‰ê°€ ì—”ì§„"""

    def __init__(self, models_config_path: Optional[str] = None, enable_debug: bool = False):
        """
        Args:
            models_config_path: models.json íŒŒì¼ ê²½ë¡œ
            enable_debug: ë””ë²„ê·¸ ë¡œê·¸ í™œì„±í™”
        """
        self.models_config_path = models_config_path or "models/models.json"
        self.models: Dict[str, BaseModel] = {}
        self.enable_debug = enable_debug
        self.logger = None
        self.passages_map: Dict[str, str] = {}  # passage_id â†’ passage_text ë§¤í•‘
        self._load_models_config()
    
    def _load_models_config(self):
        """models.jsonì—ì„œ ëª¨ë¸ ì„¤ì • ë¡œë“œ"""
        import json
        
        if not Path(self.models_config_path).exists():
            print(f"âš ï¸  ëª¨ë¸ ì„¤ì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.models_config_path}")
            return
        
        with open(self.models_config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        print(f"ğŸ“‹ ëª¨ë¸ ì„¤ì • ë¡œë“œ: {len(config.get('models', []))}ê°œ ëª¨ë¸")
    
    def create_model(self, provider: str, model_name: str, api_key: str, **kwargs) -> BaseModel:
        """ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        
        Args:
            provider: ì œê³µì (openai, anthropic, google, upstage, perplexity)
            model_name: ëª¨ë¸ ì´ë¦„
            api_key: API í‚¤
            **kwargs: ì¶”ê°€ ì„¤ì •
        
        Returns:
            BaseModel ì¸ìŠ¤í„´ìŠ¤
        """
        provider_map = {
            'openai': OpenAIModel,
            'anthropic': AnthropicModel,
            'google': GoogleModel,
            'upstage': UpstageModel,
            'perplexity': PerplexityModel
        }
        
        model_class = provider_map.get(provider.lower())
        if not model_class:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì œê³µì: {provider}")
        
        return model_class(api_key=api_key, model_name=model_name, **kwargs)
    
    def load_exam(self, exam_path: str, question_numbers: Optional[List[int]] = None) -> Dict[str, Any]:
        """ì‹œí—˜ YAML íŒŒì¼ ë¡œë“œ

        Args:
            exam_path: YAML íŒŒì¼ ê²½ë¡œ
            question_numbers: í‰ê°€í•  ë¬¸ì œ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ì „ì²´)

        Returns:
            ì‹œí—˜ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        with open(exam_path, 'r', encoding='utf-8') as f:
            exam_data = yaml.safe_load(f)

        # passages ì„¹ì…˜ì´ ìˆìœ¼ë©´ passage_id â†’ passage_text ë§¤í•‘ ìƒì„±
        if 'passages' in exam_data:
            self.passages_map = {
                p['passage_id']: p['passage_text']
                for p in exam_data['passages']
            }
        else:
            self.passages_map = {}

        # íŠ¹ì • ë¬¸ì œë§Œ í•„í„°ë§
        if question_numbers:
            original_count = len(exam_data.get('questions', []))
            exam_data['questions'] = [
                q for q in exam_data.get('questions', [])
                if q.get('question_number') in question_numbers
            ]
            filtered_count = len(exam_data['questions'])
            print(f"   ğŸ“Œ ë¬¸ì œ í•„í„°ë§: {original_count}ê°œ â†’ {filtered_count}ê°œ")

        return exam_data
    
    def _setup_logger(self, exam_id: str, model_name: str):
        """ë¡œê±° ì„¤ì • (íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í¬í•¨ëœ ë¡œê·¸ íŒŒì¼)"""
        if not self.enable_debug:
            return

        # logs ë””ë ‰í† ë¦¬ ìƒì„±
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)

        # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_filename = f"{exam_id}_{model_name.replace('/', '-')}_{timestamp}.log"
        log_path = log_dir / log_filename

        # ë¡œê±° ì„¤ì •
        self.logger = logging.getLogger(f"evaluator_{timestamp}")
        self.logger.setLevel(logging.DEBUG)
        self.logger.handlers = []  # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°

        # íŒŒì¼ í•¸ë“¤ëŸ¬ (ìƒì„¸ ë¡œê·¸)
        file_handler = logging.FileHandler(log_path, encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)
        file_formatter = logging.Formatter(
            '%(asctime)s | %(levelname)-8s | %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        file_handler.setFormatter(file_formatter)
        self.logger.addHandler(file_handler)

        # ì½˜ì†” í•¸ë“¤ëŸ¬ (ë””ë²„ê·¸ ì¶œë ¥)
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        console_formatter = logging.Formatter(
            'ğŸ› %(message)s'
        )
        console_handler.setFormatter(console_formatter)
        self.logger.addHandler(console_handler)

        print(f"ğŸ“ ë””ë²„ê·¸ ë¡œê·¸: {log_path}")
        self.logger.info(f"í‰ê°€ ì‹œì‘ - ì‹œí—˜: {exam_id}, ëª¨ë¸: {model_name}")
        self.logger.info("=" * 100)

    def _log_question_debug(self, question: Dict[str, Any], response: ModelResponse, result: Dict[str, Any]):
        """ë¬¸ì œë³„ ìƒì„¸ ë””ë²„ê·¸ ë¡œê·¸"""
        if not self.logger:
            return

        q_num = result['question_number']
        q_id = result['question_id']

        self.logger.info(f"\n{'='*100}")
        self.logger.info(f"ğŸ“ ë¬¸ì œ {q_num}ë²ˆ (ID: {q_id})")
        self.logger.info(f"{'='*100}")

        # ì§€ë¬¸ ì²˜ë¦¬: passage í•„ë“œ ìš°ì„ , ì—†ìœ¼ë©´ passage_idë¡œ ì°¸ì¡°
        passage = question.get('passage')
        if not passage and question.get('passage_id'):
            passage_id = question.get('passage_id')
            passage = self.passages_map.get(passage_id)

        # ë¬¸ì œ ì •ë³´
        self.logger.info(f"[ë¬¸ì œ ë‚´ìš©]")
        if passage:
            passage_preview = passage[:200] + "..." if len(passage) > 200 else passage
            passage_id_info = f" (passage_id: {question.get('passage_id')})" if question.get('passage_id') else ""
            self.logger.info(f"ì§€ë¬¸{passage_id_info}: {passage_preview}")
        self.logger.info(f"ì§ˆë¬¸: {question.get('question_text', '')}")
        self.logger.info(f"ì„ íƒì§€:")
        for i, choice in enumerate(question.get('choices', []), 1):
            self.logger.info(f"  {i}. {choice}")
        self.logger.info(f"ë°°ì : {result['points']}ì ")

        # API ìš”ì²­ ì •ë³´
        self.logger.info(f"\n[API ìš”ì²­]")
        self.logger.info(f"ëª¨ë¸: {response.model if hasattr(response, 'model') else 'N/A'}")
        request_summary = {
            'question_text': question.get('question_text', '')[:100] + "...",
            'choices_count': len(question.get('choices', [])),
            'has_passage': bool(passage),
            'passage_id': question.get('passage_id')
        }
        self.logger.info(f"ìš”ì²­ ì •ë³´: {json.dumps(request_summary, ensure_ascii=False, indent=2)}")

        # API ì‘ë‹µ ì •ë³´
        self.logger.info(f"\n[API ì‘ë‹µ]")
        self.logger.info(f"ì„ íƒí•œ ë‹µ: {result['answer']}ë²ˆ")
        self.logger.info(f"ì •ë‹µ: {result['correct_answer']}ë²ˆ")
        self.logger.info(f"ì •ë‹µ ì—¬ë¶€: {'âœ… ì •ë‹µ' if result['is_correct'] else 'âŒ ì˜¤ë‹µ'}")
        self.logger.info(f"ì†Œìš” ì‹œê°„: {result['time_taken']}ì´ˆ")
        self.logger.info(f"ì„±ê³µ ì—¬ë¶€: {'âœ… ì„±ê³µ' if result['success'] else 'âŒ ì‹¤íŒ¨'}")

        if result.get('error'):
            self.logger.error(f"ì—ëŸ¬ ë©”ì‹œì§€: {result['error']}")

        # ë‹µë³€ ì´ìœ 
        self.logger.info(f"\n[ë‹µë³€ ì´ìœ ]")
        reasoning_lines = result['reasoning'].split('\n')
        for line in reasoning_lines:
            self.logger.info(f"  {line}")

        # raw_responseê°€ ìˆë‹¤ë©´ ë¡œê·¸
        if hasattr(response, 'raw_response') and response.raw_response:
            self.logger.debug(f"\n[Raw Response]")
            try:
                raw_json = json.dumps(response.raw_response, ensure_ascii=False, indent=2)
                self.logger.debug(raw_json)
            except:
                self.logger.debug(str(response.raw_response))

        self.logger.info(f"{'='*100}\n")

    def _solve_single_question(self, model: BaseModel, question: Dict[str, Any], index: int) -> Dict[str, Any]:
        """ë‹¨ì¼ ë¬¸ì œ í’€ì´ (ë³‘ë ¬ ì²˜ë¦¬ìš©)

        Args:
            model: AI ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
            question: ë¬¸ì œ ë°ì´í„°
            index: ë¬¸ì œ ì¸ë±ìŠ¤ (1ë¶€í„° ì‹œì‘)

        Returns:
            ë¬¸ì œ í’€ì´ ê²°ê³¼
        """
        q_id = question.get('question_id', f'q{index}')
        q_num = question.get('question_number', index)
        q_text = question.get('question_text', '')
        choices = question.get('choices', [])

        # ì§€ë¬¸ ì²˜ë¦¬: passage í•„ë“œ ìš°ì„ , ì—†ìœ¼ë©´ passage_idë¡œ ì°¸ì¡°
        passage = question.get('passage')
        if not passage and question.get('passage_id'):
            passage_id = question.get('passage_id')
            passage = self.passages_map.get(passage_id)

        correct_answer = question.get('correct_answer')
        points = question.get('points', 2)

        # ì£¼ê´€ì‹ vs ê°ê´€ì‹ íŒë³„
        is_subjective = not choices or len(choices) == 0

        # ë“£ê¸° ë¬¸ì œ ìŠ¤í‚µ (question_textì— ì‹¤ì œ í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ê²½ìš°)
        if not q_text or q_text.strip() == '' or 'ë“£ê³ ' in q_text:
            result = {
                'question_id': q_id,
                'question_number': q_num,
                'answer': 0,
                'correct_answer': correct_answer,
                'is_correct': None,
                'reasoning': 'ë“£ê¸° í‰ê°€ ë¬¸ì œ (ìŠ¤í‚µ)',
                'time_taken': 0,
                'points': points,
                'earned_points': 0,
                'success': False,
                'error': 'ë“£ê¸° í‰ê°€ ë¬¸ì œëŠ” í‰ê°€í•˜ì§€ ì•ŠìŒ'
            }
            # subject_type ì¶”ê°€
            if 'subject_type' in question:
                result['subject_type'] = question['subject_type']
            if self.logger:
                self.logger.info(f"ë¬¸ì œ {q_num}ë²ˆ: ë“£ê¸° í‰ê°€ ë¬¸ì œ ìŠ¤í‚µ")
            return result

        # ë¬¸ì œ í’€ì´
        response = model.solve_question(
            question_text=q_text,
            choices=choices,
            passage=passage,
            is_subjective=is_subjective  # ì£¼ê´€ì‹ ì—¬ë¶€ ì „ë‹¬
        )

        # ì •ë‹µ í™•ì¸
        if is_subjective:
            # ì£¼ê´€ì‹: answerë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ
            try:
                answer_num = float(response.answer) if isinstance(response.answer, (int, float, str)) else None
                correct_num = float(correct_answer) if correct_answer is not None else None

                # ì†Œìˆ˜ì  ì´í•˜ ìë¦¿ìˆ˜ ê³ ë ¤í•˜ì—¬ ë¹„êµ (ì˜¤ì°¨ í—ˆìš©: 0.01)
                if answer_num is not None and correct_num is not None:
                    is_correct = abs(answer_num - correct_num) < 0.01
                else:
                    is_correct = False
            except (ValueError, TypeError):
                # ìˆ«ìë¡œ ë³€í™˜ ë¶ˆê°€ëŠ¥í•˜ë©´ ì˜¤ë‹µ ì²˜ë¦¬
                is_correct = False
        else:
            # ê°ê´€ì‹: ê¸°ì¡´ ë¡œì§ (1-5 ë¹„êµ)
            is_correct = (response.answer == correct_answer) if correct_answer else None

        # ê²°ê³¼ ì €ì¥
        result = {
            'question_id': q_id,
            'question_number': q_num,
            'answer': response.answer,
            'correct_answer': correct_answer,
            'is_correct': is_correct,
            'reasoning': response.reasoning,
            'time_taken': round(response.time_taken, 2),
            'points': points,
            'earned_points': points if is_correct else 0,
            'success': response.success,
            'error': response.error
        }

        # subject_type ì¶”ê°€ (ì„ íƒê³¼ëª© ì •ë³´)
        if 'subject_type' in question:
            result['subject_type'] = question['subject_type']

        # ë””ë²„ê·¸ ë¡œê·¸ ì¶œë ¥
        self._log_question_debug(question, response, result)

        return result

    def evaluate_exam(
        self,
        exam_path: str,
        model: BaseModel,
        output_path: Optional[str] = None,
        parallel: bool = False,
        max_workers: int = 10,
        question_numbers: Optional[List[int]] = None
    ) -> Dict[str, Any]:
        """ì‹œí—˜ í‰ê°€ ì‹¤í–‰

        Args:
            exam_path: ì‹œí—˜ YAML íŒŒì¼ ê²½ë¡œ
            model: AI ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
            output_path: ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
            parallel: ë³‘ë ¬ ì²˜ë¦¬ ì—¬ë¶€ (ê¸°ë³¸: False)
            max_workers: ìµœëŒ€ ë™ì‹œ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìˆ˜ (ê¸°ë³¸: 10)
            question_numbers: í‰ê°€í•  ë¬¸ì œ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ì „ì²´)

        Returns:
            í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print(f"\n{'='*100}")
        print(f"ğŸ¯ í‰ê°€ ì‹œì‘")
        print(f"{'='*100}")
        print(f"ğŸ“„ ì‹œí—˜: {exam_path}")
        print(f"ğŸ¤– ëª¨ë¸: {model.model_name}")
        if parallel:
            print(f"âš¡ ë³‘ë ¬ ì²˜ë¦¬: {max_workers}ê°œ ìŠ¤ë ˆë“œ")
        if self.enable_debug:
            print(f"ğŸ› ë””ë²„ê·¸ ëª¨ë“œ: í™œì„±í™”")
        print(f"{'='*100}\n")

        # ì‹œí—˜ ë¡œë“œ
        exam_data = self.load_exam(exam_path, question_numbers=question_numbers)
        exam_id = exam_data.get('exam_id', 'unknown')
        questions = exam_data.get('questions', [])

        print(f"ğŸ“Š ë¬¸ì œ ìˆ˜: {len(questions)}ê°œ\n")

        # ë””ë²„ê·¸ ë¡œê±° ì„¤ì •
        if self.enable_debug:
            self._setup_logger(exam_id, model.model_name)

        start_time = time.time()

        if parallel:
            # ë³‘ë ¬ ì²˜ë¦¬
            results = []
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # ëª¨ë“  ë¬¸ì œë¥¼ ë™ì‹œì— ì œì¶œ
                future_to_question = {
                    executor.submit(self._solve_single_question, model, q, i): (i, q)
                    for i, q in enumerate(questions, 1)
                }

                # ì™„ë£Œë˜ëŠ” ëŒ€ë¡œ ê²°ê³¼ ìˆ˜ì§‘
                for future in as_completed(future_to_question):
                    i, question = future_to_question[future]
                    try:
                        result = future.result()
                        results.append(result)

                        # ì§„í–‰ ìƒí™© ì¶œë ¥
                        q_num = result['question_number']
                        points = result['points']
                        is_correct = result['is_correct']
                        answer = result['answer']
                        correct_answer = result['correct_answer']
                        time_taken = result['time_taken']
                        success = result['success']

                        status = "âœ…" if is_correct else "âŒ"
                        if not success:
                            status = "âš ï¸"

                        print(f"ğŸ“ ë¬¸ì œ {q_num}ë²ˆ ({points}ì ): {status} ë‹µ: {answer}ë²ˆ (ì •ë‹µ: {correct_answer}ë²ˆ) - {time_taken:.2f}ì´ˆ")

                        if not success:
                            print(f"   âš ï¸  ì—ëŸ¬: {result['error']}")

                    except Exception as e:
                        print(f"âŒ ë¬¸ì œ {i}ë²ˆ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

            # ë¬¸ì œ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬
            results.sort(key=lambda x: x['question_number'])

        else:
            # ìˆœì°¨ ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)
            results = []
            for i, question in enumerate(questions, 1):
                q_num = question.get('question_number', i)
                points = question.get('points', 2)

                print(f"ğŸ“ ë¬¸ì œ {q_num}ë²ˆ ({points}ì )...", end=' ')

                result = self._solve_single_question(model, question, i)
                results.append(result)

                # ì§„í–‰ ìƒí™© ì¶œë ¥
                status = "âœ…" if result['is_correct'] else "âŒ"
                if not result['success']:
                    status = "âš ï¸"
                print(f"{status} ë‹µ: {result['answer']}ë²ˆ (ì •ë‹µ: {result['correct_answer']}ë²ˆ) - {result['time_taken']:.2f}ì´ˆ")

                if not result['success']:
                    print(f"   âš ï¸  ì—ëŸ¬: {result['error']}")

        total_time = time.time() - start_time

        # í†µê³„ ê³„ì‚°
        max_score = sum(r['points'] for r in results)
        correct_count = sum(1 for r in results if r['is_correct'])
        total_score = sum(r['earned_points'] for r in results)

        # ìµœì¢… ê²°ê³¼
        accuracy = (correct_count / len(questions) * 100) if questions else 0
        score_rate = (total_score / max_score * 100) if max_score > 0 else 0

        # ë¡œê·¸ì— ìµœì¢… ìš”ì•½ ê¸°ë¡
        if self.logger:
            self.logger.info("=" * 100)
            self.logger.info("ğŸ“Š í‰ê°€ ì™„ë£Œ - ìµœì¢… ìš”ì•½")
            self.logger.info("=" * 100)
            self.logger.info(f"ì •ë‹µë¥ : {correct_count}/{len(questions)} ({accuracy:.1f}%)")
            self.logger.info(f"ì ìˆ˜: {total_score}/{max_score}ì  ({score_rate:.1f}%)")
            self.logger.info(f"ì´ ì†Œìš” ì‹œê°„: {total_time:.1f}ì´ˆ")
            self.logger.info("=" * 100)

        print(f"\n{'='*100}")
        print(f"ğŸ“Š í‰ê°€ ì™„ë£Œ")
        print(f"{'='*100}")
        print(f"ì •ë‹µë¥ : {correct_count}/{len(questions)} ({accuracy:.1f}%)")
        print(f"ì ìˆ˜: {total_score}/{max_score}ì  ({score_rate:.1f}%)")
        print(f"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {total_time:.1f}ì´ˆ")
        if parallel:
            avg_time_per_question = sum(r['time_taken'] for r in results) / len(results) if results else 0
            speedup = (avg_time_per_question * len(results)) / total_time if total_time > 0 else 1
            print(f"âš¡ ì†ë„ í–¥ìƒ: {speedup:.1f}ë°° (ë³‘ë ¬ ì²˜ë¦¬)")
        print(f"{'='*100}\n")
        
        # ê²°ê³¼ ë°ì´í„° êµ¬ì„±
        result_data = {
            'exam_id': exam_id,
            'exam_title': exam_data.get('title', ''),
            'subject': exam_data.get('subject', ''),
            'model_name': model.model_name,
            'evaluated_at': datetime.now().isoformat(),
            'summary': {
                'total_questions': len(questions),
                'correct_answers': correct_count,
                'accuracy': round(accuracy, 2),
                'total_score': total_score,
                'max_score': max_score,
                'score_rate': round(score_rate, 2)
            },
            'results': results
        }
        
        # ê²°ê³¼ ì €ì¥
        if output_path is None:
            # ìë™ ê²½ë¡œ ìƒì„±: results/{exam_id}/{model_name}.yaml
            results_dir = Path("results") / exam_id
            results_dir.mkdir(parents=True, exist_ok=True)
            output_path = results_dir / f"{model.model_name.replace('/', '-')}.yaml"
        else:
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            yaml.dump(result_data, f, allow_unicode=True, default_flow_style=False, sort_keys=False)
        
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}\n")
        
        return result_data
    
    def evaluate_with_all_models(
        self,
        exam_path: str,
        models_to_use: Optional[List[str]] = None,
        question_numbers: Optional[List[int]] = None
    ) -> Dict[str, Dict[str, Any]]:
        """ëª¨ë“  ëª¨ë¸ë¡œ ì‹œí—˜ í‰ê°€

        Args:
            exam_path: ì‹œí—˜ YAML íŒŒì¼ ê²½ë¡œ
            models_to_use: ì‚¬ìš©í•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ì „ì²´)
            question_numbers: í‰ê°€í•  ë¬¸ì œ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ì „ì²´)

        Returns:
            {model_name: result_data} ë”•ì…”ë„ˆë¦¬
        """
        import json

        # ëª¨ë¸ ì„¤ì • ë¡œë“œ
        with open(self.models_config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)

        models = config.get('models', [])
        if models_to_use:
            models = [m for m in models if m['name'] in models_to_use]

        print(f"\nğŸš€ ì „ì²´ ëª¨ë¸ í‰ê°€ ì‹œì‘ ({len(models)}ê°œ ëª¨ë¸)")
        print(f"ğŸ“„ ì‹œí—˜: {exam_path}\n")

        results = {}

        for model_config in models:
            provider = model_config['provider']
            model_name = model_config['name']
            model_id = model_config.get('model_id', model_name)  # model_idê°€ ì—†ìœ¼ë©´ name ì‚¬ìš©

            # API í‚¤ ê°€ì ¸ì˜¤ê¸°
            api_key_var = f"{provider.upper()}_API_KEY"
            api_key = os.getenv(api_key_var)

            if not api_key:
                print(f"âš ï¸  {model_name} ìŠ¤í‚µ: {api_key_var} í™˜ê²½ë³€ìˆ˜ ì—†ìŒ")
                continue

            try:
                # ëª¨ë¸ ì„¤ì •ì—ì„œ ì¶”ê°€ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
                model_kwargs = {}
                for key in ['max_tokens', 'temperature', 'timeout', 'top_p', 'top_k']:
                    if key in model_config:
                        model_kwargs[key] = model_config[key]

                # ëª¨ë¸ ìƒì„± (model_idë¥¼ ì „ë‹¬)
                model = self.create_model(
                    provider=provider,
                    model_name=model_id,  # APIìš© ì‹¤ì œ model_id ì „ë‹¬
                    api_key=api_key,
                    **model_kwargs  # models.jsonì˜ ì„¤ì • ì „ë‹¬
                )
                # í‘œì‹œìš© ì´ë¦„ì€ model_name ì‚¬ìš©
                model.display_name = model_name

                # í‰ê°€ ì‹¤í–‰
                result = self.evaluate_exam(exam_path, model, question_numbers=question_numbers)
                results[model_name] = result
                
            except Exception as e:
                print(f"âŒ {model_name} í‰ê°€ ì‹¤íŒ¨: {e}")
                continue
        
        print(f"\nğŸ‰ ì „ì²´ í‰ê°€ ì™„ë£Œ! ({len(results)}/{len(models)}ê°œ ì„±ê³µ)\n")
        
        return results

