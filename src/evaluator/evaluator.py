#!/usr/bin/env python3
"""
í‰ê°€ ì—”ì§„

YAML ì‹œí—˜ íŒŒì¼ì„ ë¡œë“œí•˜ê³  AI ëª¨ë¸ë¡œ ë¬¸ì œë¥¼ í’€ì–´ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
"""

import yaml
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import os

from .base_model import BaseModel, ModelResponse
from .models import (
    OpenAIModel,
    AnthropicModel,
    GoogleModel,
    UpstageModel,
    PerplexityModel
)


class Evaluator:
    """í‰ê°€ ì—”ì§„"""
    
    def __init__(self, models_config_path: Optional[str] = None):
        """
        Args:
            models_config_path: models.json íŒŒì¼ ê²½ë¡œ
        """
        self.models_config_path = models_config_path or "models/models.json"
        self.models: Dict[str, BaseModel] = {}
        self._load_models_config()
    
    def _load_models_config(self):
        """models.jsonì—ì„œ ëª¨ë¸ ì„¤ì • ë¡œë“œ"""
        import json
        
        if not Path(self.models_config_path).exists():
            print(f"âš ï¸  ëª¨ë¸ ì„¤ì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.models_config_path}")
            return
        
        with open(self.models_config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        print(f"ğŸ“‹ ëª¨ë¸ ì„¤ì • ë¡œë“œ: {len(config.get('models', []))}ê°œ ëª¨ë¸")
    
    def create_model(self, provider: str, model_name: str, api_key: str, **kwargs) -> BaseModel:
        """ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        
        Args:
            provider: ì œê³µì (openai, anthropic, google, upstage, perplexity)
            model_name: ëª¨ë¸ ì´ë¦„
            api_key: API í‚¤
            **kwargs: ì¶”ê°€ ì„¤ì •
        
        Returns:
            BaseModel ì¸ìŠ¤í„´ìŠ¤
        """
        provider_map = {
            'openai': OpenAIModel,
            'anthropic': AnthropicModel,
            'google': GoogleModel,
            'upstage': UpstageModel,
            'perplexity': PerplexityModel
        }
        
        model_class = provider_map.get(provider.lower())
        if not model_class:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì œê³µì: {provider}")
        
        return model_class(api_key=api_key, model_name=model_name, **kwargs)
    
    def load_exam(self, exam_path: str) -> Dict[str, Any]:
        """ì‹œí—˜ YAML íŒŒì¼ ë¡œë“œ
        
        Args:
            exam_path: YAML íŒŒì¼ ê²½ë¡œ
        
        Returns:
            ì‹œí—˜ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        with open(exam_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def evaluate_exam(
        self,
        exam_path: str,
        model: BaseModel,
        output_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """ì‹œí—˜ í‰ê°€ ì‹¤í–‰
        
        Args:
            exam_path: ì‹œí—˜ YAML íŒŒì¼ ê²½ë¡œ
            model: AI ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
            output_path: ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
        
        Returns:
            í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print(f"\n{'='*100}")
        print(f"ğŸ¯ í‰ê°€ ì‹œì‘")
        print(f"{'='*100}")
        print(f"ğŸ“„ ì‹œí—˜: {exam_path}")
        print(f"ğŸ¤– ëª¨ë¸: {model.model_name}")
        print(f"{'='*100}\n")
        
        # ì‹œí—˜ ë¡œë“œ
        exam_data = self.load_exam(exam_path)
        exam_id = exam_data.get('exam_id', 'unknown')
        questions = exam_data.get('questions', [])
        
        print(f"ğŸ“Š ë¬¸ì œ ìˆ˜: {len(questions)}ê°œ")
        
        # ê° ë¬¸ì œ í’€ì´
        results = []
        correct_count = 0
        total_score = 0
        max_score = 0
        
        for i, question in enumerate(questions, 1):
            q_id = question.get('question_id', f'q{i}')
            q_num = question.get('question_number', i)
            q_text = question.get('question_text', '')
            choices = question.get('choices', [])
            passage = question.get('passage')
            correct_answer = question.get('correct_answer')
            points = question.get('points', 2)
            
            max_score += points
            
            print(f"\nğŸ“ ë¬¸ì œ {q_num}ë²ˆ ({points}ì )...")
            
            # ë¬¸ì œ í’€ì´
            response = model.solve_question(
                question_text=q_text,
                choices=choices,
                passage=passage
            )
            
            # ì •ë‹µ í™•ì¸
            is_correct = (response.answer == correct_answer) if correct_answer else None
            if is_correct:
                correct_count += 1
                total_score += points
            
            # ê²°ê³¼ ì €ì¥
            result = {
                'question_id': q_id,
                'question_number': q_num,
                'answer': response.answer,
                'correct_answer': correct_answer,
                'is_correct': is_correct,
                'reasoning': response.reasoning,
                'time_taken': round(response.time_taken, 2),
                'points': points,
                'earned_points': points if is_correct else 0,
                'success': response.success,
                'error': response.error
            }
            results.append(result)
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            status = "âœ…" if is_correct else "âŒ"
            if not response.success:
                status = "âš ï¸"
            print(f"  {status} ë‹µ: {response.answer}ë²ˆ (ì •ë‹µ: {correct_answer}ë²ˆ) - {response.time_taken:.2f}ì´ˆ")
            if not response.success:
                print(f"     ì—ëŸ¬: {response.error}")
        
        # ìµœì¢… ê²°ê³¼
        accuracy = (correct_count / len(questions) * 100) if questions else 0
        score_rate = (total_score / max_score * 100) if max_score > 0 else 0
        
        print(f"\n{'='*100}")
        print(f"ğŸ“Š í‰ê°€ ì™„ë£Œ")
        print(f"{'='*100}")
        print(f"ì •ë‹µë¥ : {correct_count}/{len(questions)} ({accuracy:.1f}%)")
        print(f"ì ìˆ˜: {total_score}/{max_score}ì  ({score_rate:.1f}%)")
        print(f"{'='*100}\n")
        
        # ê²°ê³¼ ë°ì´í„° êµ¬ì„±
        result_data = {
            'exam_id': exam_id,
            'exam_title': exam_data.get('title', ''),
            'subject': exam_data.get('subject', ''),
            'model_name': model.model_name,
            'evaluated_at': datetime.now().isoformat(),
            'summary': {
                'total_questions': len(questions),
                'correct_answers': correct_count,
                'accuracy': round(accuracy, 2),
                'total_score': total_score,
                'max_score': max_score,
                'score_rate': round(score_rate, 2)
            },
            'results': results
        }
        
        # ê²°ê³¼ ì €ì¥
        if output_path is None:
            # ìë™ ê²½ë¡œ ìƒì„±: results/{exam_id}/{model_name}.yaml
            results_dir = Path("results") / exam_id
            results_dir.mkdir(parents=True, exist_ok=True)
            output_path = results_dir / f"{model.model_name.replace('/', '-')}.yaml"
        else:
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            yaml.dump(result_data, f, allow_unicode=True, default_flow_style=False, sort_keys=False)
        
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}\n")
        
        return result_data
    
    def evaluate_with_all_models(
        self,
        exam_path: str,
        models_to_use: Optional[List[str]] = None
    ) -> Dict[str, Dict[str, Any]]:
        """ëª¨ë“  ëª¨ë¸ë¡œ ì‹œí—˜ í‰ê°€
        
        Args:
            exam_path: ì‹œí—˜ YAML íŒŒì¼ ê²½ë¡œ
            models_to_use: ì‚¬ìš©í•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ì „ì²´)
        
        Returns:
            {model_name: result_data} ë”•ì…”ë„ˆë¦¬
        """
        import json
        
        # ëª¨ë¸ ì„¤ì • ë¡œë“œ
        with open(self.models_config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        models = config.get('models', [])
        if models_to_use:
            models = [m for m in models if m['name'] in models_to_use]
        
        print(f"\nğŸš€ ì „ì²´ ëª¨ë¸ í‰ê°€ ì‹œì‘ ({len(models)}ê°œ ëª¨ë¸)")
        print(f"ğŸ“„ ì‹œí—˜: {exam_path}\n")
        
        results = {}
        
        for model_config in models:
            provider = model_config['provider']
            model_name = model_config['name']
            
            # API í‚¤ ê°€ì ¸ì˜¤ê¸°
            api_key_var = f"{provider.upper()}_API_KEY"
            api_key = os.getenv(api_key_var)
            
            if not api_key:
                print(f"âš ï¸  {model_name} ìŠ¤í‚µ: {api_key_var} í™˜ê²½ë³€ìˆ˜ ì—†ìŒ")
                continue
            
            try:
                # ëª¨ë¸ ìƒì„±
                model = self.create_model(
                    provider=provider,
                    model_name=model_name,
                    api_key=api_key
                )
                
                # í‰ê°€ ì‹¤í–‰
                result = self.evaluate_exam(exam_path, model)
                results[model_name] = result
                
            except Exception as e:
                print(f"âŒ {model_name} í‰ê°€ ì‹¤íŒ¨: {e}")
                continue
        
        print(f"\nğŸ‰ ì „ì²´ í‰ê°€ ì™„ë£Œ! ({len(results)}/{len(models)}ê°œ ì„±ê³µ)\n")
        
        return results

