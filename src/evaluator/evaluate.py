#!/usr/bin/env python3
"""
í‰ê°€ CLI ë„êµ¬

ì‚¬ìš©ë²•:
    python evaluate.py <exam_yaml> [ì˜µì…˜]
    python evaluate.py --all  # ëª¨ë“  ì‹œí—˜ í‰ê°€
"""

import argparse
import sys
from pathlib import Path
import os
from typing import List

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.evaluator.evaluator import Evaluator


def parse_question_numbers(question_spec: str) -> List[int]:
    """ë¬¸ì œ ë²ˆí˜¸ ë²”ìœ„ íŒŒì‹±

    Args:
        question_spec: ë¬¸ì œ ë²ˆí˜¸ ì§€ì • (ì˜ˆ: "1-5", "1,3,5", "1-3,7,10-12")

    Returns:
        ë¬¸ì œ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸

    Examples:
        >>> parse_question_numbers("1-5")
        [1, 2, 3, 4, 5]
        >>> parse_question_numbers("1,3,5")
        [1, 3, 5]
        >>> parse_question_numbers("1-3,7,10-12")
        [1, 2, 3, 7, 10, 11, 12]
    """
    question_numbers = set()

    for part in question_spec.split(','):
        part = part.strip()
        if '-' in part:
            # ë²”ìœ„ ì§€ì • (ì˜ˆ: "1-5")
            start, end = part.split('-')
            question_numbers.update(range(int(start), int(end) + 1))
        else:
            # ë‹¨ì¼ ë²ˆí˜¸ (ì˜ˆ: "3")
            question_numbers.add(int(part))

    return sorted(list(question_numbers))


def main():
    parser = argparse.ArgumentParser(
        description='KSAT AI í‰ê°€ ë„êµ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # íŠ¹ì • ì‹œí—˜ í‰ê°€ (OpenAI GPT-4o)
  %(prog)s exams/parsed/2025-math-sat-p1-2.yaml --model gpt-4o

  # íŠ¹ì • ë¬¸ì œë§Œ í‰ê°€ (1-5ë²ˆ)
  %(prog)s exams/parsed/2025-korean-sat.yaml --model gpt-5 --questions "1-5"

  # ì—¬ëŸ¬ ë¬¸ì œ ì„ íƒ (1, 3, 5ë²ˆ)
  %(prog)s exams/parsed/2025-korean-sat.yaml --model gpt-5 -q "1,3,5"

  # ë²”ìœ„ ì¡°í•© (1-3ë²ˆ, 7ë²ˆ, 10-12ë²ˆ)
  %(prog)s exams/parsed/2025-korean-sat.yaml --model gpt-5 -q "1-3,7,10-12"

  # ëª¨ë“  ëª¨ë¸ë¡œ í‰ê°€
  %(prog)s exams/parsed/2025-korean-sat.yaml --all-models

  # ëª¨ë“  ì‹œí—˜ í‰ê°€
  %(prog)s --all

  # íŠ¹ì • ì œê³µì ëª¨ë¸ë“¤ë§Œ
  %(prog)s exams/parsed/2025-math-sat-p1-2.yaml --provider openai
        """
    )
    
    parser.add_argument(
        'exam',
        nargs='?',
        type=str,
        help='í‰ê°€í•  ì‹œí—˜ YAML íŒŒì¼ ê²½ë¡œ'
    )
    
    parser.add_argument(
        '--all',
        action='store_true',
        help='exams/parsed/ì˜ ëª¨ë“  ì‹œí—˜ í‰ê°€'
    )
    
    parser.add_argument(
        '--model',
        type=str,
        help='ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ (ì˜ˆ: gpt-4o, claude-3-5-sonnet-20241022)'
    )
    
    parser.add_argument(
        '--provider',
        type=str,
        choices=['openai', 'anthropic', 'google', 'upstage', 'perplexity'],
        help='íŠ¹ì • ì œê³µìì˜ ëª¨ë¸ë“¤ë§Œ ì‚¬ìš©'
    )
    
    parser.add_argument(
        '--all-models',
        action='store_true',
        help='ëª¨ë“  ëª¨ë¸ë¡œ í‰ê°€'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        help='ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ê¸°ë³¸: results/{exam_id}/{model_name}.yaml)'
    )
    
    parser.add_argument(
        '--models-config',
        type=str,
        default='models/models.json',
        help='ëª¨ë¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸: models/models.json)'
    )

    parser.add_argument(
        '--parallel',
        action='store_true',
        help='ë³‘ë ¬ ì²˜ë¦¬ë¡œ í‰ê°€ (ì†ë„ í–¥ìƒ)'
    )

    parser.add_argument(
        '--max-workers',
        type=int,
        default=10,
        help='ë³‘ë ¬ ì²˜ë¦¬ ì‹œ ìµœëŒ€ ë™ì‹œ ìŠ¤ë ˆë“œ ìˆ˜ (ê¸°ë³¸: 10)'
    )

    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        dest='verbose',
        help='ìƒì„¸ ë¡œê·¸ í™œì„±í™” (ë¬¸ì œë³„ API ìš”ì²­/ì‘ë‹µ, logs/ ë””ë ‰í† ë¦¬ì— ì €ì¥)'
    )

    parser.add_argument(
        '--debug',
        action='store_true',
        dest='verbose',
        help='--verboseì˜ ë³„ì¹­ (í•˜ìœ„ í˜¸í™˜ì„±)'
    )

    parser.add_argument(
        '--questions', '-q',
        type=str,
        help='í‰ê°€í•  ë¬¸ì œ ë²ˆí˜¸ ì§€ì • (ì˜ˆ: "1-5", "1,3,5", "1-3,7-10")'
    )

    args = parser.parse_args()
    
    # í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
    try:
        from dotenv import load_dotenv
        load_dotenv()
        print("âœ… .env íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
    except ImportError:
        print("âš ï¸  python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ ì‚¬ìš©í•˜ë ¤ë©´ 'pip install python-dotenv'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    except Exception as e:
        print(f"âš ï¸  .env íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")

    # ë¬¸ì œ ë²ˆí˜¸ íŒŒì‹±
    question_numbers = None
    if args.questions:
        try:
            question_numbers = parse_question_numbers(args.questions)
            print(f"ğŸ“ í‰ê°€í•  ë¬¸ì œ: {question_numbers} ({len(question_numbers)}ê°œ)")
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: ë¬¸ì œ ë²ˆí˜¸ íŒŒì‹± ì‹¤íŒ¨: {e}")
            print(f"   ì˜¬ë°”ë¥¸ í˜•ì‹: '1-5', '1,3,5', '1-3,7-10' ë“±")
            sys.exit(1)

    # í‰ê°€ê¸° ìƒì„±
    evaluator = Evaluator(models_config_path=args.models_config, enable_debug=args.verbose)
    
    # ì‹œí—˜ íŒŒì¼ ëª©ë¡ ìƒì„±
    exam_files = []
    
    if args.all:
        # exams/parsed/ì˜ ëª¨ë“  YAML íŒŒì¼
        parsed_dir = Path('exams/parsed')
        if parsed_dir.exists():
            exam_files = list(parsed_dir.glob('*.yaml'))
        else:
            print(f"âŒ ì˜¤ë¥˜: {parsed_dir} ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)
    elif args.exam:
        exam_files = [Path(args.exam)]
    else:
        print("âŒ ì˜¤ë¥˜: ì‹œí—˜ íŒŒì¼ì„ ì§€ì •í•˜ê±°ë‚˜ --all ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        parser.print_help()
        sys.exit(1)
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    for exam_file in exam_files:
        if not exam_file.exists():
            print(f"âŒ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {exam_file}")
            sys.exit(1)
    
    print(f"\n{'='*100}")
    print(f"ğŸ¯ KSAT AI í‰ê°€ ì‹œì‘")
    print(f"{'='*100}")
    print(f"ğŸ“„ ì‹œí—˜ íŒŒì¼: {len(exam_files)}ê°œ")
    print(f"{'='*100}\n")
    
    # í‰ê°€ ì‹¤í–‰
    for exam_file in exam_files:
        print(f"\nğŸ“š ì‹œí—˜: {exam_file.name}")
        
        if args.all_models:
            # ëª¨ë“  ëª¨ë¸ë¡œ í‰ê°€
            models_to_use = None
            if args.provider:
                # íŠ¹ì • ì œê³µìë§Œ
                import json
                with open(args.models_config, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                models_to_use = [
                    m['name'] for m in config.get('models', [])
                    if m['provider'] == args.provider
                ]
            
            evaluator.evaluate_with_all_models(
                exam_path=str(exam_file),
                models_to_use=models_to_use,
                question_numbers=question_numbers
            )
        
        elif args.model:
            # íŠ¹ì • ëª¨ë¸ë¡œ í‰ê°€
            import json
            with open(args.models_config, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # ëª¨ë¸ ì„¤ì • ì°¾ê¸°
            model_config = None
            for m in config.get('models', []):
                if m['name'] == args.model:
                    model_config = m
                    break
            
            if not model_config:
                print(f"âŒ ì˜¤ë¥˜: ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.model}")
                print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸:")
                for m in config.get('models', []):
                    print(f"  - {m['name']} ({m['provider']})")
                sys.exit(1)
            
            # API í‚¤ ê°€ì ¸ì˜¤ê¸°
            provider = model_config['provider']
            model_id = model_config.get('model_id', args.model)  # model_idê°€ ì—†ìœ¼ë©´ name ì‚¬ìš©
            api_key_var = f"{provider.upper()}_API_KEY"
            api_key = os.getenv(api_key_var)
            
            if not api_key:
                print(f"âŒ ì˜¤ë¥˜: {api_key_var} í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                sys.exit(1)
            
            # ëª¨ë¸ ìƒì„± (model_id ì‚¬ìš©)
            # model_configì—ì„œ ì¶”ê°€ ì„¤ì • ì¶”ì¶œ (max_tokens, temperature ë“±)
            model_kwargs = {}
            for key in ['max_tokens', 'temperature', 'timeout', 'top_p', 'top_k']:
                if key in model_config:
                    model_kwargs[key] = model_config[key]

            model = evaluator.create_model(
                provider=provider,
                model_name=model_id,  # APIìš© ì‹¤ì œ model_id ì „ë‹¬
                api_key=api_key,
                **model_kwargs  # models.jsonì˜ ì„¤ì • ì „ë‹¬
            )
            # í‘œì‹œìš© ì´ë¦„
            model.display_name = args.model
            
            # í‰ê°€ ì‹¤í–‰
            evaluator.evaluate_exam(
                exam_path=str(exam_file),
                model=model,
                output_path=args.output,
                parallel=args.parallel,
                max_workers=args.max_workers,
                question_numbers=question_numbers
            )
        
        else:
            # ê¸°ë³¸: OpenAI GPT-4o
            api_key = os.getenv('OPENAI_API_KEY')
            if not api_key:
                print("âŒ ì˜¤ë¥˜: OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                print("   --model ë˜ëŠ” --all-models ì˜µì…˜ì„ ì‚¬ìš©í•˜ê±°ë‚˜ .env íŒŒì¼ì„ ì„¤ì •í•˜ì„¸ìš”.")
                sys.exit(1)
            
            model = evaluator.create_model(
                provider='openai',
                model_name='gpt-4o',
                api_key=api_key
            )

            evaluator.evaluate_exam(
                exam_path=str(exam_file),
                model=model,
                output_path=args.output,
                parallel=args.parallel,
                max_workers=args.max_workers,
                question_numbers=question_numbers
            )
    
    print(f"\n{'='*100}")
    print(f"ğŸ‰ ëª¨ë“  í‰ê°€ ì™„ë£Œ!")
    print(f"{'='*100}")
    print(f"ğŸ“ ê²°ê³¼ ë””ë ‰í† ë¦¬: results/")
    print(f"\në‹¤ìŒ ë‹¨ê³„:")
    print(f"  1. ê²°ê³¼ í™•ì¸: ls -la results/*/")
    print(f"  2. Git ì»¤ë°‹: git add results/ && git commit -m 'feat: í‰ê°€ ê²°ê³¼ ì¶”ê°€'")
    print(f"  3. GitHub Pages ë°°í¬")
    print(f"{'='*100}\n")


if __name__ == "__main__":
    main()

