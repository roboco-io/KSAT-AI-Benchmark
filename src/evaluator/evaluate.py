#!/usr/bin/env python3
"""
í‰ê°€ CLI ë„êµ¬

ì‚¬ìš©ë²•:
    python evaluate.py <exam_yaml> [ì˜µì…˜]
    python evaluate.py --all  # ëª¨ë“  ì‹œí—˜ í‰ê°€
"""

import argparse
import sys
from pathlib import Path
import os

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.evaluator.evaluator import Evaluator


def main():
    parser = argparse.ArgumentParser(
        description='KSAT AI í‰ê°€ ë„êµ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # íŠ¹ì • ì‹œí—˜ í‰ê°€ (OpenAI GPT-4o)
  %(prog)s exams/parsed/2025-math-sat-p1-2.yaml --model gpt-4o
  
  # ëª¨ë“  ëª¨ë¸ë¡œ í‰ê°€
  %(prog)s exams/parsed/2025-korean-sat.yaml --all-models
  
  # ëª¨ë“  ì‹œí—˜ í‰ê°€
  %(prog)s --all
  
  # íŠ¹ì • ì œê³µì ëª¨ë¸ë“¤ë§Œ
  %(prog)s exams/parsed/2025-math-sat-p1-2.yaml --provider openai
        """
    )
    
    parser.add_argument(
        'exam',
        nargs='?',
        type=str,
        help='í‰ê°€í•  ì‹œí—˜ YAML íŒŒì¼ ê²½ë¡œ'
    )
    
    parser.add_argument(
        '--all',
        action='store_true',
        help='exams/parsed/ì˜ ëª¨ë“  ì‹œí—˜ í‰ê°€'
    )
    
    parser.add_argument(
        '--model',
        type=str,
        help='ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ (ì˜ˆ: gpt-4o, claude-3-5-sonnet-20241022)'
    )
    
    parser.add_argument(
        '--provider',
        type=str,
        choices=['openai', 'anthropic', 'google', 'upstage', 'perplexity'],
        help='íŠ¹ì • ì œê³µìì˜ ëª¨ë¸ë“¤ë§Œ ì‚¬ìš©'
    )
    
    parser.add_argument(
        '--all-models',
        action='store_true',
        help='ëª¨ë“  ëª¨ë¸ë¡œ í‰ê°€'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        help='ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ê¸°ë³¸: results/{exam_id}/{model_name}.yaml)'
    )
    
    parser.add_argument(
        '--models-config',
        type=str,
        default='models/models.json',
        help='ëª¨ë¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸: models/models.json)'
    )

    parser.add_argument(
        '--parallel',
        action='store_true',
        help='ë³‘ë ¬ ì²˜ë¦¬ë¡œ í‰ê°€ (ì†ë„ í–¥ìƒ)'
    )

    parser.add_argument(
        '--max-workers',
        type=int,
        default=10,
        help='ë³‘ë ¬ ì²˜ë¦¬ ì‹œ ìµœëŒ€ ë™ì‹œ ìŠ¤ë ˆë“œ ìˆ˜ (ê¸°ë³¸: 10)'
    )

    args = parser.parse_args()
    
    # í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
    try:
        from dotenv import load_dotenv
        load_dotenv()
        print("âœ… .env íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
    except ImportError:
        print("âš ï¸  python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ ì‚¬ìš©í•˜ë ¤ë©´ 'pip install python-dotenv'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    except Exception as e:
        print(f"âš ï¸  .env íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # í‰ê°€ê¸° ìƒì„±
    evaluator = Evaluator(models_config_path=args.models_config)
    
    # ì‹œí—˜ íŒŒì¼ ëª©ë¡ ìƒì„±
    exam_files = []
    
    if args.all:
        # exams/parsed/ì˜ ëª¨ë“  YAML íŒŒì¼
        parsed_dir = Path('exams/parsed')
        if parsed_dir.exists():
            exam_files = list(parsed_dir.glob('*.yaml'))
        else:
            print(f"âŒ ì˜¤ë¥˜: {parsed_dir} ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)
    elif args.exam:
        exam_files = [Path(args.exam)]
    else:
        print("âŒ ì˜¤ë¥˜: ì‹œí—˜ íŒŒì¼ì„ ì§€ì •í•˜ê±°ë‚˜ --all ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        parser.print_help()
        sys.exit(1)
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    for exam_file in exam_files:
        if not exam_file.exists():
            print(f"âŒ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {exam_file}")
            sys.exit(1)
    
    print(f"\n{'='*100}")
    print(f"ğŸ¯ KSAT AI í‰ê°€ ì‹œì‘")
    print(f"{'='*100}")
    print(f"ğŸ“„ ì‹œí—˜ íŒŒì¼: {len(exam_files)}ê°œ")
    print(f"{'='*100}\n")
    
    # í‰ê°€ ì‹¤í–‰
    for exam_file in exam_files:
        print(f"\nğŸ“š ì‹œí—˜: {exam_file.name}")
        
        if args.all_models:
            # ëª¨ë“  ëª¨ë¸ë¡œ í‰ê°€
            models_to_use = None
            if args.provider:
                # íŠ¹ì • ì œê³µìë§Œ
                import json
                with open(args.models_config, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                models_to_use = [
                    m['name'] for m in config.get('models', [])
                    if m['provider'] == args.provider
                ]
            
            evaluator.evaluate_with_all_models(
                exam_path=str(exam_file),
                models_to_use=models_to_use
            )
        
        elif args.model:
            # íŠ¹ì • ëª¨ë¸ë¡œ í‰ê°€
            import json
            with open(args.models_config, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # ëª¨ë¸ ì„¤ì • ì°¾ê¸°
            model_config = None
            for m in config.get('models', []):
                if m['name'] == args.model:
                    model_config = m
                    break
            
            if not model_config:
                print(f"âŒ ì˜¤ë¥˜: ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.model}")
                print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸:")
                for m in config.get('models', []):
                    print(f"  - {m['name']} ({m['provider']})")
                sys.exit(1)
            
            # API í‚¤ ê°€ì ¸ì˜¤ê¸°
            provider = model_config['provider']
            model_id = model_config.get('model_id', args.model)  # model_idê°€ ì—†ìœ¼ë©´ name ì‚¬ìš©
            api_key_var = f"{provider.upper()}_API_KEY"
            api_key = os.getenv(api_key_var)
            
            if not api_key:
                print(f"âŒ ì˜¤ë¥˜: {api_key_var} í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                sys.exit(1)
            
            # ëª¨ë¸ ìƒì„± (model_id ì‚¬ìš©)
            model = evaluator.create_model(
                provider=provider,
                model_name=model_id,  # APIìš© ì‹¤ì œ model_id ì „ë‹¬
                api_key=api_key
            )
            # í‘œì‹œìš© ì´ë¦„
            model.display_name = args.model
            
            # í‰ê°€ ì‹¤í–‰
            evaluator.evaluate_exam(
                exam_path=str(exam_file),
                model=model,
                output_path=args.output,
                parallel=args.parallel,
                max_workers=args.max_workers
            )
        
        else:
            # ê¸°ë³¸: OpenAI GPT-4o
            api_key = os.getenv('OPENAI_API_KEY')
            if not api_key:
                print("âŒ ì˜¤ë¥˜: OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                print("   --model ë˜ëŠ” --all-models ì˜µì…˜ì„ ì‚¬ìš©í•˜ê±°ë‚˜ .env íŒŒì¼ì„ ì„¤ì •í•˜ì„¸ìš”.")
                sys.exit(1)
            
            model = evaluator.create_model(
                provider='openai',
                model_name='gpt-4o',
                api_key=api_key
            )
            
            evaluator.evaluate_exam(
                exam_path=str(exam_file),
                model=model,
                output_path=args.output,
                parallel=args.parallel,
                max_workers=args.max_workers
            )
    
    print(f"\n{'='*100}")
    print(f"ğŸ‰ ëª¨ë“  í‰ê°€ ì™„ë£Œ!")
    print(f"{'='*100}")
    print(f"ğŸ“ ê²°ê³¼ ë””ë ‰í† ë¦¬: results/")
    print(f"\në‹¤ìŒ ë‹¨ê³„:")
    print(f"  1. ê²°ê³¼ í™•ì¸: ls -la results/*/")
    print(f"  2. Git ì»¤ë°‹: git add results/ && git commit -m 'feat: í‰ê°€ ê²°ê³¼ ì¶”ê°€'")
    print(f"  3. GitHub Pages ë°°í¬")
    print(f"{'='*100}\n")


if __name__ == "__main__":
    main()

