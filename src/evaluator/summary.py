#!/usr/bin/env python3
"""
í‰ê°€ ê²°ê³¼ ìš”ì•½ ë° ë¶„ì„ ë„êµ¬

results/ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  í‰ê°€ ê²°ê³¼ë¥¼ ì½ì–´ì„œ ìš”ì•½ í…Œì´ë¸”ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
"""

import yaml
from pathlib import Path
from typing import Dict, List, Any, Optional
from collections import defaultdict
import argparse
import sys


def load_results() -> Dict[str, List[Dict]]:
    """ëª¨ë“  í‰ê°€ ê²°ê³¼ ë¡œë“œ
    
    Returns:
        {exam_id: [result1, result2, ...]} ë”•ì…”ë„ˆë¦¬
    """
    results_dir = Path("results")
    if not results_dir.exists():
        print("âŒ results/ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {}
    
    results = defaultdict(list)
    
    for exam_dir in results_dir.iterdir():
        if not exam_dir.is_dir():
            continue
        
        exam_id = exam_dir.name
        
        for result_file in exam_dir.glob("*.yaml"):
            with open(result_file, 'r', encoding='utf-8') as f:
                result_data = yaml.safe_load(f)
                results[exam_id].append(result_data)
    
    return dict(results)


def print_summary(results: Dict[str, List[Dict]]):
    """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    
    print("\n" + "="*100)
    print("ğŸ“Š KSAT AI Benchmark - í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print("="*100)
    
    for exam_id, exam_results in results.items():
        if not exam_results:
            continue
        
        # ì²« ë²ˆì§¸ ê²°ê³¼ì—ì„œ ì‹œí—˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        first_result = exam_results[0]
        exam_title = first_result.get('exam_title', exam_id)
        subject = first_result.get('subject', 'unknown')
        
        print(f"\nğŸ“š {exam_title}")
        print(f"   ê³¼ëª©: {subject.upper()}")
        print(f"   í‰ê°€ ëª¨ë¸: {len(exam_results)}ê°œ")
        print()
        
        # í…Œì´ë¸” í—¤ë”
        print(f"{'ëª¨ë¸':<30} {'ì •ë‹µë¥ ':<15} {'ì ìˆ˜':<15} {'í‰ê°€ ì‹œê°„':<15}")
        print("-" * 80)
        
        # ì •í™•ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_results = sorted(
            exam_results,
            key=lambda x: x['summary']['accuracy'],
            reverse=True
        )
        
        for result in sorted_results:
            model_name = result['model_name']
            summary = result['summary']
            
            accuracy = summary['accuracy']
            correct = summary['correct_answers']
            total = summary['total_questions']
            score = summary['total_score']
            max_score = summary['max_score']
            
            # í‰ê°€ ì‹œê°„ ê³„ì‚°
            total_time = sum(r['time_taken'] for r in result['results'])
            
            print(f"{model_name:<30} {accuracy:>5.1f}% ({correct}/{total}){'':<3} {score:>3}/{max_score:<3}ì {'':<4} {total_time:>6.1f}ì´ˆ")
    
    print("\n" + "="*100)
    
    # ì „ì²´ í†µê³„
    total_exams = len(results)
    total_evaluations = sum(len(r) for r in results.values())
    
    print(f"ğŸ“ˆ ì „ì²´ í†µê³„")
    print(f"   - í‰ê°€ëœ ì‹œí—˜: {total_exams}ê°œ")
    print(f"   - ì´ í‰ê°€ íšŸìˆ˜: {total_evaluations}íšŒ")
    print("="*100 + "\n")


def print_leaderboard(results: Dict[str, List[Dict]]):
    """ì „ì²´ ë¦¬ë”ë³´ë“œ ì¶œë ¥ (ëª¨ë¸ë³„ í‰ê·  ì„±ì )"""

    print("\n" + "="*100)
    print("ğŸ† ì „ì²´ ë¦¬ë”ë³´ë“œ (ëª¨ë¸ë³„ í‰ê·  ì„±ì )")
    print("="*100)

    # ëª¨ë¸ë³„ í†µê³„ ìˆ˜ì§‘
    model_stats = defaultdict(lambda: {
        'total_accuracy': 0,
        'total_score_rate': 0,
        'total_time': 0,
        'count': 0,
        'exams': []
    })

    for exam_id, exam_results in results.items():
        for result in exam_results:
            model_name = result['model_name']
            summary = result['summary']

            model_stats[model_name]['total_accuracy'] += summary['accuracy']
            model_stats[model_name]['total_score_rate'] += summary.get('score_rate', 0)
            model_stats[model_name]['total_time'] += sum(r['time_taken'] for r in result['results'])
            model_stats[model_name]['count'] += 1
            model_stats[model_name]['exams'].append(exam_id)

    # í‰ê·  ê³„ì‚° ë° ì •ë ¬
    leaderboard = []
    for model_name, stats in model_stats.items():
        count = stats['count']
        leaderboard.append({
            'model': model_name,
            'avg_accuracy': stats['total_accuracy'] / count,
            'avg_score_rate': stats['total_score_rate'] / count,
            'avg_time': stats['total_time'] / count,
            'eval_count': count,
            'exams': stats['exams']
        })

    # ì •í™•ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    leaderboard.sort(key=lambda x: x['avg_accuracy'], reverse=True)

    # í…Œì´ë¸” ì¶œë ¥
    print(f"\n{'ìˆœìœ„':<6} {'ëª¨ë¸':<30} {'í‰ê·  ì •ë‹µë¥ ':<15} {'í‰ê·  ì ìˆ˜ìœ¨':<15} {'í‰ê·  ì‹œê°„':<15} {'í‰ê°€ íšŸìˆ˜':<10}")
    print("-" * 100)

    for rank, entry in enumerate(leaderboard, 1):
        medal = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰" if rank == 3 else f"{rank:2d}."
        print(f"{medal:<6} {entry['model']:<30} {entry['avg_accuracy']:>6.2f}%{'':<8} "
              f"{entry['avg_score_rate']:>6.2f}%{'':<8} {entry['avg_time']:>7.1f}ì´ˆ{'':<6} "
              f"{entry['eval_count']:>3}íšŒ")

    print("="*100)


def print_subject_analysis(results: Dict[str, List[Dict]]):
    """ê³¼ëª©ë³„ ë¶„ì„"""

    print("\n" + "="*100)
    print("ğŸ“Š ê³¼ëª©ë³„ ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„")
    print("="*100)

    # ê³¼ëª©ë³„ë¡œ ê²°ê³¼ ë¶„ë¥˜
    subject_results = defaultdict(lambda: defaultdict(list))

    for exam_id, exam_results in results.items():
        for result in exam_results:
            subject = result.get('subject', 'unknown')
            model_name = result['model_name']
            subject_results[subject][model_name].append(result)

    # ê³¼ëª©ë³„ ì¶œë ¥
    for subject, models in sorted(subject_results.items()):
        subject_name = {
            'korean': 'êµ­ì–´',
            'math': 'ìˆ˜í•™',
            'english': 'ì˜ì–´',
            'science': 'ê³¼í•™',
            'social': 'ì‚¬íšŒ'
        }.get(subject, subject.upper())

        print(f"\nğŸ“š {subject_name}")
        print(f"{'ëª¨ë¸':<30} {'í‰ê·  ì •ë‹µë¥ ':<15} {'í‰ê·  ì ìˆ˜ìœ¨':<15}")
        print("-" * 65)

        # ëª¨ë¸ë³„ í‰ê·  ê³„ì‚°
        model_avgs = []
        for model_name, model_results in models.items():
            avg_accuracy = sum(r['summary']['accuracy'] for r in model_results) / len(model_results)
            avg_score_rate = sum(r['summary'].get('score_rate', 0) for r in model_results) / len(model_results)
            model_avgs.append({
                'model': model_name,
                'accuracy': avg_accuracy,
                'score_rate': avg_score_rate
            })

        # ì •í™•ë„ ìˆœ ì •ë ¬
        model_avgs.sort(key=lambda x: x['accuracy'], reverse=True)

        for entry in model_avgs:
            print(f"{entry['model']:<30} {entry['accuracy']:>6.2f}%{'':<8} {entry['score_rate']:>6.2f}%")

    print("\n" + "="*100)


def print_detailed_stats(results: Dict[str, List[Dict]]):
    """ìƒì„¸ í†µê³„"""

    print("\n" + "="*100)
    print("ğŸ“ˆ ìƒì„¸ í†µê³„")
    print("="*100)

    total_exams = len(results)
    total_evaluations = sum(len(r) for r in results.values())

    # ëª¨ë¸ë³„ ì¹´ìš´íŠ¸
    model_counts = defaultdict(int)
    for exam_results in results.values():
        for result in exam_results:
            model_counts[result['model_name']] += 1

    print(f"\nì´ í‰ê°€ëœ ì‹œí—˜: {total_exams}ê°œ")
    print(f"ì´ í‰ê°€ íšŸìˆ˜: {total_evaluations}íšŒ")
    print(f"í‰ê°€ëœ ëª¨ë¸ ìˆ˜: {len(model_counts)}ê°œ")

    print(f"\nëª¨ë¸ë³„ í‰ê°€ íšŸìˆ˜:")
    for model, count in sorted(model_counts.items(), key=lambda x: x[1], reverse=True):
        print(f"  - {model}: {count}íšŒ")

    print("="*100)


def filter_results(results: Dict[str, List[Dict]], model: Optional[str] = None,
                   subject: Optional[str] = None, year: Optional[str] = None) -> Dict[str, List[Dict]]:
    """ê²°ê³¼ í•„í„°ë§"""

    filtered = defaultdict(list)

    for exam_id, exam_results in results.items():
        for result in exam_results:
            # ëª¨ë¸ í•„í„°
            if model and result['model_name'] != model:
                continue

            # ê³¼ëª© í•„í„°
            if subject and result.get('subject') != subject:
                continue

            # ì—°ë„ í•„í„°
            if year and not exam_id.startswith(year):
                continue

            filtered[exam_id].append(result)

    return dict(filtered)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description='KSAT AI Benchmark í‰ê°€ ê²°ê³¼ ìš”ì•½ ë° ë¶„ì„',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  %(prog)s                          - ì „ì²´ ìš”ì•½
  %(prog)s --detailed               - ìƒì„¸ ë¶„ì„ í¬í•¨
  %(prog)s --leaderboard            - ë¦¬ë”ë³´ë“œë§Œ í‘œì‹œ
  %(prog)s --subject korean         - êµ­ì–´ ê³¼ëª©ë§Œ ë¶„ì„
  %(prog)s --model gpt-5            - íŠ¹ì • ëª¨ë¸ë§Œ ë¶„ì„
  %(prog)s --year 2025              - íŠ¹ì • ì—°ë„ë§Œ ë¶„ì„
        """
    )

    parser.add_argument(
        '--detailed', '-d',
        action='store_true',
        help='ìƒì„¸ ë¶„ì„ í¬í•¨ (ê³¼ëª©ë³„, í†µê³„ ë“±)'
    )

    parser.add_argument(
        '--leaderboard', '-l',
        action='store_true',
        help='ë¦¬ë”ë³´ë“œë§Œ í‘œì‹œ'
    )

    parser.add_argument(
        '--model', '-m',
        type=str,
        help='íŠ¹ì • ëª¨ë¸ë§Œ ë¶„ì„'
    )

    parser.add_argument(
        '--subject', '-s',
        type=str,
        choices=['korean', 'math', 'english', 'science', 'social'],
        help='íŠ¹ì • ê³¼ëª©ë§Œ ë¶„ì„'
    )

    parser.add_argument(
        '--year', '-y',
        type=str,
        help='íŠ¹ì • ì—°ë„ë§Œ ë¶„ì„ (ì˜ˆ: 2025)'
    )

    args = parser.parse_args()

    # ê²°ê³¼ ë¡œë“œ
    results = load_results()

    if not results:
        print("âš ï¸  í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("   í‰ê°€ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”: make gpt-5 2025 korean")
        return

    # í•„í„° ì ìš©
    if args.model or args.subject or args.year:
        results = filter_results(results, args.model, args.subject, args.year)
        if not results:
            print("âš ï¸  í•„í„° ì¡°ê±´ì— ë§ëŠ” ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

    # ì¶œë ¥
    if args.leaderboard:
        print_leaderboard(results)
    elif args.detailed:
        print_summary(results)
        print_leaderboard(results)
        print_subject_analysis(results)
        print_detailed_stats(results)
    else:
        print_summary(results)
        print_leaderboard(results)


if __name__ == "__main__":
    main()

