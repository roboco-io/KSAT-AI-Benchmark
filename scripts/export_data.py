#!/usr/bin/env python3
"""
í‰ê°€ ê²°ê³¼ë¥¼ ì›¹ìš© JSONìœ¼ë¡œ export

ì›¹ í”„ë¡ íŠ¸ì—”ë“œê°€ staticí•˜ê²Œ ë¡œë“œí•  ìˆ˜ ìˆë„ë¡ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
"""

import json
import sys
import yaml
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.evaluator.summary import load_results


def load_enabled_models():
    """models.jsonì—ì„œ í™œì„±í™”ëœ ëª¨ë¸ ëª©ë¡ ë¡œë“œ"""
    models_file = project_root / 'models' / 'models.json'
    enabled_models = set()

    try:
        with open(models_file, 'r', encoding='utf-8') as f:
            models_data = json.load(f)
            for model in models_data.get('models', []):
                if model.get('enabled', False):
                    enabled_models.add(model['name'])
    except Exception as e:
        print(f"âš ï¸  models.json ë¡œë“œ ì‹¤íŒ¨: {e}")

    return enabled_models


def load_exam_data():
    """ì‹œí—˜ ë¬¸ì œ ë°ì´í„° ë¡œë“œ"""
    exams_dir = project_root / 'exams' / 'parsed'
    exams = {}
    
    if not exams_dir.exists():
        return exams
    
    for yaml_file in exams_dir.glob('*.yaml'):
        try:
            with open(yaml_file, 'r', encoding='utf-8') as f:
                exam_data = yaml.safe_load(f)
                if exam_data and 'exam_id' in exam_data:
                    exams[exam_data['exam_id']] = exam_data
        except Exception as e:
            print(f"âš ï¸  {yaml_file.name} ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return exams


def export_to_json():
    """í‰ê°€ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ export"""

    print("ğŸ“¤ í‰ê°€ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ export ì¤‘...")

    # í™œì„±í™”ëœ ëª¨ë¸ ëª©ë¡ ë¡œë“œ
    enabled_models = load_enabled_models()
    print(f"   - í™œì„±í™”ëœ ëª¨ë¸: {len(enabled_models)}ê°œ")
    if enabled_models:
        print(f"     {', '.join(sorted(enabled_models))}")

    # ê²°ê³¼ ë¡œë“œ
    results = load_results()

    # ì‹œí—˜ ë¬¸ì œ ë°ì´í„° ë¡œë“œ
    exams = load_exam_data()

    if not results:
        print("âš ï¸  í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ë¦¬ë”ë³´ë“œ ìƒì„±
    leaderboard = []
    all_results_list = []

    for exam_id, exam_results in results.items():
        for result in exam_results:
            all_results_list.append(result)

    # ëª¨ë¸ë³„ë¡œ ê·¸ë£¹í™” (í™œì„±í™”ëœ ëª¨ë¸ë§Œ)
    model_map = {}
    for result in all_results_list:
        model_name = result['model_name']
        # enabled ëª¨ë¸ë§Œ í¬í•¨
        if enabled_models and model_name not in enabled_models:
            continue
        if model_name not in model_map:
            model_map[model_name] = []
        model_map[model_name].append(result)
    
    # ê³¼ëª©ë³„ë¡œ ê²°ê³¼ ë¶„ë¥˜
    def create_leaderboard(model_map_filtered):
        """ì£¼ì–´ì§„ ëª¨ë¸ ë§µìœ¼ë¡œ ë¦¬ë”ë³´ë“œ ìƒì„±"""
        board = []
        for model_name, model_results in model_map_filtered.items():
            if not model_results:
                continue

            total_questions = sum(r['summary']['total_questions'] for r in model_results)
            correct_answers = sum(r['summary']['correct_answers'] for r in model_results)
            total_score = sum(r['summary']['total_score'] for r in model_results)
            max_score = sum(r['summary']['max_score'] for r in model_results)
            total_time = sum(
                sum(q['time_taken'] for q in r['results'])
                for r in model_results
            )

            accuracy = (correct_answers / total_questions * 100) if total_questions > 0 else 0
            score_rate = (total_score / max_score * 100) if max_score > 0 else 0
            avg_time = total_time / total_questions if total_questions > 0 else 0

            board.append({
                'model_name': model_name,
                'accuracy': round(accuracy, 2),
                'score_rate': round(score_rate, 2),
                'total_score': total_score,
                'max_score': max_score,
                'correct_answers': correct_answers,
                'avg_time': round(avg_time, 2),
                'exams_count': len(model_results),
            })

        board.sort(key=lambda x: x['accuracy'], reverse=True)
        return board

    # ì „ì²´ ë¦¬ë”ë³´ë“œ
    leaderboard = create_leaderboard(model_map)

    # ê³¼ëª©ë³„ ë¦¬ë”ë³´ë“œ ìƒì„±
    subject_leaderboards = {}
    for subject in ['korean', 'math', 'english']:
        subject_model_map = {}
        for model_name, model_results in model_map.items():
            subject_results = [r for r in model_results if r.get('subject') == subject]
            if subject_results:
                subject_model_map[model_name] = subject_results

        if subject_model_map:
            subject_leaderboards[subject] = create_leaderboard(subject_model_map)
    
    # í†µê³„
    total_exams = len(results)
    total_evaluations = len(all_results_list)

    # JSON ë°ì´í„° ìƒì„±
    data = {
        'leaderboard': leaderboard,
        'leaderboardBySubject': subject_leaderboards,  # ê³¼ëª©ë³„ ë¦¬ë”ë³´ë“œ ì¶”ê°€
        'stats': {
            'totalExams': total_exams,
            'totalEvaluations': total_evaluations,
        },
        'results': all_results_list,
        'exams': exams,  # ì‹œí—˜ ë¬¸ì œ ë°ì´í„° ì¶”ê°€
    }
    
    # web/public/data/ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = project_root / 'web' / 'public' / 'data'
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    output_file = output_dir / 'evaluation-data.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… Export ì™„ë£Œ: {output_file}")
    print(f"   - ë¦¬ë”ë³´ë“œ ì—”íŠ¸ë¦¬: {len(leaderboard)}ê°œ")
    print(f"   - ì „ì²´ í‰ê°€ ê²°ê³¼: {total_evaluations}ê°œ")


if __name__ == '__main__':
    export_to_json()

